{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b58aef9-5f34-4b3a-95ff-cfe9d06eb67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 83954655232 (78.19 GB)\n",
      "Present Capacity: 57933582336 (53.95 GB)\n",
      "DFS Remaining: 57933557760 (53.95 GB)\n",
      "DFS Used: 24576 (24 KB)\n",
      "DFS Used%: 0.00%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 172.18.0.2:9866 (p4-hdfs-1.p4_default)\n",
      "Hostname: main\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 83954655232 (78.19 GB)\n",
      "DFS Used: 24576 (24 KB)\n",
      "Non DFS Used: 21709381632 (20.22 GB)\n",
      "DFS Remaining: 57933557760 (53.95 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 69.01%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Fri Jan 05 08:59:47 GMT 2024\n",
      "Last Block Report: Fri Jan 05 08:57:35 GMT 2024\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfsadmin -fs hdfs://main:9000 -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e826e984-79ff-4c41-afd0-e49ec1b02224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: /hadoop-3.3.6/logs does not exist. Creating.\n",
      "Usage: hdfs namenode [-backup] | \n",
      "\t[-checkpoint] | \n",
      "\t[-format [-clusterid cid ] [-force] [-nonInteractive] ] | \n",
      "\t[-upgrade [-clusterid cid] [-renameReserved<k-v pairs>] ] | \n",
      "\t[-upgradeOnly [-clusterid cid] [-renameReserved<k-v pairs>] ] | \n",
      "\t[-rollback] | \n",
      "\t[-rollingUpgrade <rollback|started> ] | \n",
      "\t[-importCheckpoint] | \n",
      "\t[-initializeSharedEdits] | \n",
      "\t[-bootstrapStandby [-force] [-nonInteractive] [-skipSharedEditsCheck] ] | \n",
      "\t[-recover [ -force] ] | \n",
      "\t[-metadataVersion ]\n",
      "\n",
      "Generic options supported are:\n",
      "-conf <configuration file>        specify an application configuration file\n",
      "-D <property=value>               define a value for a given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
      "\n",
      "The general command line syntax is:\n",
      "command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs namenode --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e333d2-4736-4807-83af-75a30b419b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--help: Unknown command\n",
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile [-n] <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum [-v] <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-concat <target path> <src path> <src path> ...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-f] [-p] [-crc] [-ignoreCrc] [-t <thread count>] [-q <thread pool queue size>] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] [-s] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] [-t <thread count>] [-q <thread pool queue size>] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] [-v] [-x] <path> ...]\n",
      "\t[-expunge [-immediate] [-fs <path>]]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-f] [-p] [-crc] [-ignoreCrc] [-t <thread count>] [-q <thread pool queue size>] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\n",
      "\t[-head <file>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] [-s <sleep interval>] <file>]\n",
      "\t[-test -[defswrz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touch [-a] [-m] [-t TIMESTAMP (yyyyMMdd:HHmmss) ] [-c] <path> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are:\n",
      "-conf <configuration file>        specify an application configuration file\n",
      "-D <property=value>               define a value for a given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
      "\n",
      "The general command line syntax is:\n",
      "command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c97f9e3-38cf-4c37-98d5-cd7ffc49f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir hdfs://main:9000/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac9ce000-47fa-4d22-a589-1e199c445074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat /hadoop-3.3.6/LICENSE.txt\n",
    "!hdfs dfs -cp /hadoop-3.3.6/LICENSE.txt hdfs://main:9000/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fbb69ba-57e0-4cae-a0cb-ab46aa7f1df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - root supergroup          0 2024-01-05 20:11 hdfs://main:9000/data\n",
      "Found 1 items\n",
      "-rw-r--r--   3 root supergroup      15217 2024-01-05 20:11 hdfs://main:9000/data/LICENSE.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls hdfs://main:9000/\n",
    "!hdfs dfs -ls hdfs://main:9000/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcbfeda9-c39d-4713-aa4a-5b64a7191599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15217  45651  hdfs://main:9000/data/LICENSE.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -du hdfs://main:9000/data/LICENSE.txt # 3 replicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfdb6841-1a6e-4df9-815b-17da44b5f34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "45651 / 15217  # 3 replicas, by default, \n",
    "# it wants to have 3 replicas, so this is the size it will take up\n",
    "# but currently since we only have one data node rn, so, no replicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20b36d2a-500f-4e47-8366-fd1b7845d866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://main:9870/fsck?ugi=root&path=%2Fdata%2FLICENSE.txt\n",
      "FSCK started by root (auth:SIMPLE) from /172.18.0.3 for path /data/LICENSE.txt at Fri Jan 05 21:02:42 GMT 2024\n",
      "\n",
      "\n",
      "/data/LICENSE.txt:  Under replicated BP-1086888341-172.18.0.2-1704444740925:blk_1073741825_1001. Target Replicas is 3 but found 1 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t1\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t0\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t15217 B\n",
      " Total files:\t1\n",
      " Total blocks (validated):\t1 (avg. block size 15217 B)\n",
      " Minimally replicated blocks:\t1 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t1 (100.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t3\n",
      " Average block replication:\t1.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t2 (66.666664 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Fri Jan 05 21:02:42 GMT 2024 in 17 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/data/LICENSE.txt' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "!hdfs fsck hdfs://main:9000/data/LICENSE.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ba3d224-5108-4750-bc98-6156d4f78590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hdfs dfs OPTIONS_TO_JAVA -cp /hadoop-3.3.6/LICENSE.txt hdfs://main:9000/data/\n",
    "!hdfs dfs -D dfs.replication=1  -cp /hadoop-3.3.6/LICENSE.txt hdfs://main:9000/data/v2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b892fad2-8794-4d04-bf36-28333aef440f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://main:9870/fsck?ugi=root&path=%2Fdata%2Fv2.txt\n",
      "FSCK started by root (auth:SIMPLE) from /172.18.0.3 for path /data/v2.txt at Fri Jan 05 21:06:49 GMT 2024\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t1\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t0\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t15217 B\n",
      " Total files:\t1\n",
      " Total blocks (validated):\t1 (avg. block size 15217 B)\n",
      " Minimally replicated blocks:\t1 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t3\n",
      " Average block replication:\t1.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Fri Jan 05 21:06:49 GMT 2024 in 1 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/data/v2.txt' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "!hdfs fsck hdfs://main:9000/data/v2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5427ac27-8b83-44f2-9370-509f2e01e38d",
   "metadata": {},
   "source": [
    "# WebHDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f901992a-0079-41d8-9ffb-58f313372915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "\u001b[1mDate\u001b[0m: Fri, 05 Jan 2024 21:49:53 GMT\n",
      "\u001b[1mCache-Control\u001b[0m: no-cache\n",
      "\u001b[1mExpires\u001b[0m: Fri, 05 Jan 2024 21:49:54 GMT\n",
      "\u001b[1mDate\u001b[0m: Fri, 05 Jan 2024 21:49:54 GMT\n",
      "\u001b[1mPragma\u001b[0m: no-cache\n",
      "\u001b[1mX-Content-Type-Options\u001b[0m: nosniff\n",
      "\u001b[1mX-FRAME-OPTIONS\u001b[0m: SAMEORIGIN\n",
      "\u001b[1mX-XSS-Protection\u001b[0m: 1; mode=block\n",
      "\u001b[1mContent-Type\u001b[0m: application/json\n",
      "\u001b[1mTransfer-Encoding\u001b[0m: chunked\n",
      "\n",
      "{\"FileStatuses\":{\"FileStatus\":[\n",
      "{\"accessTime\":1704485466362,\"blockSize\":134217728,\"childrenNum\":0,\"fileId\":16387,\"group\":\"supergroup\",\"length\":15217,\"modificationTime\":1704485466904,\"owner\":\"root\",\"pathSuffix\":\"LICENSE.txt\",\"permission\":\"644\",\"replication\":3,\"storagePolicy\":0,\"type\":\"FILE\"},\n",
      "{\"accessTime\":1704488793131,\"blockSize\":134217728,\"childrenNum\":0,\"fileId\":16388,\"group\":\"supergroup\",\"length\":15217,\"modificationTime\":1704488793194,\"owner\":\"root\",\"pathSuffix\":\"v2.txt\",\"permission\":\"644\",\"replication\":1,\"storagePolicy\":0,\"type\":\"FILE\"}\n",
      "]}}\n"
     ]
    }
   ],
   "source": [
    "# list dir data\n",
    "# -i means with headers\n",
    "!curl -i \"http://main:9870/webhdfs/v1/data?op=LISTSTATUS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cbca94f-bb2c-42d4-acd7-9d91402f3ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 307 Temporary Redirect\n",
      "\u001b[1mDate\u001b[0m: Fri, 05 Jan 2024 21:55:51 GMT\n",
      "\u001b[1mCache-Control\u001b[0m: no-cache\n",
      "\u001b[1mExpires\u001b[0m: Fri, 05 Jan 2024 21:55:51 GMT\n",
      "\u001b[1mDate\u001b[0m: Fri, 05 Jan 2024 21:55:51 GMT\n",
      "\u001b[1mPragma\u001b[0m: no-cache\n",
      "\u001b[1mX-Content-Type-Options\u001b[0m: nosniff\n",
      "\u001b[1mX-FRAME-OPTIONS\u001b[0m: SAMEORIGIN\n",
      "\u001b[1mX-XSS-Protection\u001b[0m: 1; mode=block\n",
      "\u001b[1mLocation\u001b[0m: http://main:9864/webhdfs/v1/data/v2.txt?op=OPEN&namenoderpcaddress=main:9000&length=200&offset=50\n",
      "\u001b[1mContent-Type\u001b[0m: application/octet-stream\n",
      "\u001b[1mContent-Length\u001b[0m: 0\n",
      "\n",
      "HTTP/1.1 200 OK\n",
      "\u001b[1mAccess-Control-Allow-Methods\u001b[0m: GET\n",
      "\u001b[1mAccess-Control-Allow-Origin\u001b[0m: *\n",
      "\u001b[1mContent-Type\u001b[0m: application/octet-stream\n",
      "\u001b[1mConnection\u001b[0m: close\n",
      "\u001b[1mContent-Length\u001b[0m: 200\n",
      "\n",
      "                          Version 2.0, January 2004\n",
      "                        http://www.apache.org/licenses/\n",
      "\n",
      "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
      "\n",
      "   1. Definitions.\n",
      "\n",
      "      "
     ]
    }
   ],
   "source": [
    "# read file v2.txt\n",
    "# -L mean follow redirects, we need this since NameNode redirects the data to a DataNode\n",
    "!curl -i -L \"http://main:9870/webhdfs/v1/data/v2.txt?op=OPEN&offset=50&length=200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8d39525-fadf-4da2-a33f-356afccc0b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 307 Temporary Redirect\n",
      "\u001b[1mDate\u001b[0m: Fri, 05 Jan 2024 21:57:50 GMT\n",
      "\u001b[1mCache-Control\u001b[0m: no-cache\n",
      "\u001b[1mExpires\u001b[0m: Fri, 05 Jan 2024 21:57:50 GMT\n",
      "\u001b[1mDate\u001b[0m: Fri, 05 Jan 2024 21:57:50 GMT\n",
      "\u001b[1mPragma\u001b[0m: no-cache\n",
      "\u001b[1mX-Content-Type-Options\u001b[0m: nosniff\n",
      "\u001b[1mX-FRAME-OPTIONS\u001b[0m: SAMEORIGIN\n",
      "\u001b[1mX-XSS-Protection\u001b[0m: 1; mode=block\n",
      "\u001b[1mLocation\u001b[0m: http://main:9864/webhdfs/v1/data/v2.txt?op=OPEN&namenoderpcaddress=main:9000&length=200&offset=50\n",
      "\u001b[1mContent-Type\u001b[0m: application/octet-stream\n",
      "\u001b[1mContent-Length\u001b[0m: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!curl -i \"http://main:9870/webhdfs/v1/data/v2.txt?op=OPEN&offset=50&length=200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c77afb0-e4d1-4b34-b401-a61f9de6e656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Location\":\"http://main:9864/webhdfs/v1/data/v2.txt?op=OPEN&namenoderpcaddress=main:9000&length=200&offset=50\"}"
     ]
    }
   ],
   "source": [
    "!curl \"http://main:9870/webhdfs/v1/data/v2.txt?op=OPEN&offset=50&length=200&noredirect=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52a6ff96-5f45-4985-a5c7-86fe7868b352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl: (52) Empty reply from server\n"
     ]
    }
   ],
   "source": [
    "# why can't I access data node directly\n",
    "!curl -i \"http://main:9866/webhdfs/v1/data/v2.txt?op=OPEN&namenoderpcaddress=main:9000&offset=50&length=200\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb11b85-b3b1-46e0-97f6-ac047599c7a5",
   "metadata": {},
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b88dbf4-28c6-4f79-967b-3beb7f01efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6cbf847-2e74-4cd3-b8ff-8e8a53c42284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(\"http://main:9870/webhdfs/v1/data/v2.txt?op=OPEN&offset=50&length=200&noredirect=true\")\n",
    "r.raise_for_status() # have and exception if we don't get code 200 (success)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ace3d877-c4f2-49b6-82f3-ba55529a20ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Location': 'http://main:9864/webhdfs/v1/data/v2.txt?op=OPEN&namenoderpcaddress=main:9000&length=200&offset=50'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d988bc3-9731-40a8-a220-fabb54628889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://main:9864/webhdfs/v1/data/v2.txt?op=OPEN&namenoderpcaddress=main:9000&length=200&offset=50'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()[\"Location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96362d2d-71dc-4847-afc2-3add5efcf70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(\"http://main:9870/webhdfs/v1/data/v2.txt?op=OPEN&offset=50&length=200\")\n",
    "r.raise_for_status() # have and exception if we don't get code 200 (success)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65a00cdc-b241-4aa0-85d8-a24c64be5a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'                          Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b4ebd-0827-4be4-b950-2d3e0c5dbeb7",
   "metadata": {},
   "source": [
    "## PyArrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee97952c-3ee8-49fa-a899-5aa278a59d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5aec5c7d-bc26-43c6-b8c2-53057cf1d03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 22:06:40,789 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "hdfs = pa.fs.HadoopFileSystem(\"main\", 9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59cc7f4b-44b9-4d5d-8814-3771d2dd14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = hdfs.open_input_file(\"/data/v2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2199193-a1af-4c9e-82ab-5b82b887d814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.lib.NativeFile,\n",
       " (pyarrow.lib.NativeFile, pyarrow.lib._Weakrefable, object))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(f), type(f).__mro__ # method resolution order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb82c108-a48d-4a41-8b95-b720f67fd944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__next__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_assert_open',\n",
       " '_assert_readable',\n",
       " '_assert_seekable',\n",
       " '_assert_writable',\n",
       " '_default_chunk_size',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'download',\n",
       " 'fileno',\n",
       " 'flush',\n",
       " 'get_stream',\n",
       " 'isatty',\n",
       " 'metadata',\n",
       " 'mode',\n",
       " 'read',\n",
       " 'read1',\n",
       " 'read_at',\n",
       " 'read_buffer',\n",
       " 'readable',\n",
       " 'readall',\n",
       " 'readinto',\n",
       " 'readline',\n",
       " 'readlines',\n",
       " 'seek',\n",
       " 'seekable',\n",
       " 'size',\n",
       " 'tell',\n",
       " 'truncate',\n",
       " 'upload',\n",
       " 'writable',\n",
       " 'write',\n",
       " 'writelines']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(f) # show all attributes and methods of f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "beed6a69-b366-4216-97d7-c8b5b3094590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15217"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee1b2076-71e1-42af-8936-d2f0ef918f6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/io.pxi:572\u001b[0m, in \u001b[0;36mpyarrow.lib.NativeFile.__next__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/io.pxi:555\u001b[0m, in \u001b[0;36mpyarrow.lib.NativeFile.readline\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnsupportedOperation\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for line in f:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6561ec3d-0083-40bd-b498-d181bc66ba92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that i'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.read_at(500, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1abf8fb-cdef-4721-b68c-1801dac7968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e22abebe-d41c-4b86-b089-489caf069078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n'\n",
      "b'                                 Apache License\\n'\n",
      "b'                           Version 2.0, January 2004\\n'\n",
      "b'                        http://www.apache.org/licenses/\\n'\n",
      "b'\\n'\n",
      "b'   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n'\n",
      "b'\\n'\n",
      "b'   1. Definitions.\\n'\n",
      "b'\\n'\n",
      "b'      \"License\" shall mean the terms and conditions for use, reproduction,\\n'\n",
      "b'      and distribution as defined by Sections 1 through 9 of this document.\\n'\n",
      "b'\\n'\n"
     ]
    }
   ],
   "source": [
    "with hdfs.open_input_file(\"/data/v2.txt\") as f:\n",
    "    reader = io.BufferedReader(f)\n",
    "    for i, line in enumerate(reader):\n",
    "        print(line)\n",
    "        if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a71820d-d37e-4c0f-ac7c-1e51142bad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                 Apache License\n",
      "                           Version 2.0, January 2004\n",
      "                        http://www.apache.org/licenses/\n",
      "\n",
      "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with hdfs.open_input_file(\"/data/v2.txt\") as f:\n",
    "    reader = io.TextIOWrapper(f)\n",
    "    for i, line in enumerate(reader):\n",
    "        print(line, end=\"\")\n",
    "        if i > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c3d545-2f29-4c35-bbc9-39a5fe691c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
