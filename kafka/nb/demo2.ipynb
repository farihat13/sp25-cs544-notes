{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e12da1-6216-41e1-beec-eae2d52c4e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m grpc_tools.protoc -I=. --python_out=. animals.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e74c81-d584-4504-9ef5-2c89227d3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from animals_pb2 import Sighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0f722f-060b-4236-b28c-b8cd86a6bc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beach: \"A\"\n",
       "animal: \"shark\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sighting(animal=\"shark\", beach=\"A\")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc5daff3-d47f-433f-8933-e074145b1035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\x01A\\x12\\x05shark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a60cea48-9cd2-4635-a822-b5a8992f6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c1a5535-36c1-416c-8d70-7493dbf26338",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = \"localhost:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdec839a-2f6b-4e41-8d79-05271c12fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin = KafkaAdminClient(bootstrap_servers=[broker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59b0baaa-8ddc-41d3-9091-851009b672bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import NewTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7243d8c5-cdb0-4bee-8b0e-1c77a610fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.errors import TopicAlreadyExistsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "207f3f33-3edd-4039-9ed2-6a606e854e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    admin.create_topics([NewTopic(\"animals\", 4, 1)])   # protobufs\n",
    "except TopicAlreadyExistsError:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    admin.create_topics([NewTopic(\"animals-json\", 4, 1)])   # JSON\n",
    "except TopicAlreadyExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf52c987-d4c2-4077-a72b-aada380c81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, threading\n",
    "\n",
    "def animal_gen():\n",
    "    producer = KafkaProducer(bootstrap_servers=[broker])\n",
    "\n",
    "    while True:\n",
    "        beach = random.choice(list(\"ABCDEFGHI\"))\n",
    "        animal = random.choice([\"shark\", \"dolphin\", \"turtle\", \"seagull\"])\n",
    "        s = Sighting(animal=animal, beach=beach)\n",
    "        producer.send(\"animals\", value=s.SerializeToString(), key=bytes(beach, \"utf-8\"))\n",
    "        time.sleep(1)\n",
    "\n",
    "threading.Thread(target=animal_gen).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e042e6-56ec-41c6-bf38-032b7bb899fb",
   "metadata": {},
   "source": [
    "# Streaming Group By (count occurences per beach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e7199cf-5c69-40fd-9794-721649a71f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread, Lock\n",
    "\n",
    "lock = Lock()\n",
    "def Print(*args):\n",
    "    with lock:\n",
    "        print(*args)\n",
    "\n",
    "Print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35ba5b05-d29f-42cb-9d35-161a74e149a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import TopicPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bfd1425-ae07-4c09-85d4-e56ad659abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beach_consumer(partitions=[]):\n",
    "    counts = {}   # key=beach, value=count\n",
    "    \n",
    "    consumer = KafkaConsumer(bootstrap_servers=[broker])\n",
    "    consumer.assign([TopicPartition(\"animals\", p) for p in partitions])\n",
    "    consumer.seek_to_beginning()\n",
    "    for i in range(10):      # TODO: loop forever\n",
    "        batch = consumer.poll(1000)\n",
    "        for tp, messages in batch.items():\n",
    "            for msg in messages:\n",
    "                s = Sighting.FromString(msg.value)\n",
    "\n",
    "                if not s.beach in counts:\n",
    "                    counts[s.beach] = 0\n",
    "                counts[s.beach] += 1\n",
    "        Print(partitions, counts)\n",
    "threading.Thread(target=beach_consumer, args=([0,1],)).start()\n",
    "threading.Thread(target=beach_consumer, args=([2,3],)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "263169bc-4cd5-4989-afa4-d978c7afb149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beach_consumer([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96189610-b7c7-4f65-bd7f-850b3dcc6275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3] {'G': 119, 'H': 127, 'A': 142, 'F': 112}\n",
      "[2, 3] {'G': 119, 'H': 127, 'A': 161, 'F': 124}\n",
      "[0, 1] {'I': 126, 'B': 125, 'C': 115, 'D': 78, 'E': 56}\n",
      "[0, 1] {'I': 126, 'B': 125, 'C': 115, 'D': 154, 'E': 118}\n",
      "[0, 1] {'shark': 119, 'turtle': 124, 'dolphin': 132, 'seagull': 125}\n",
      "[0, 1] {'shark': 153, 'turtle': 155, 'dolphin': 164, 'seagull': 166}\n",
      "[2, 3] {'dolphin': 148, 'seagull': 104, 'turtle': 136, 'shark': 112}\n",
      "[2, 3] {'dolphin': 155, 'seagull': 110, 'turtle': 148, 'shark': 118}\n",
      "[0, 1] {'shark': 153, 'turtle': 156, 'dolphin': 164, 'seagull': 166}\n",
      "[0, 1] {'I': 126, 'B': 126, 'C': 115, 'D': 154, 'E': 118}\n",
      "[2, 3] {'G': 119, 'H': 127, 'A': 161, 'F': 124}\n",
      "[2, 3] {'dolphin': 155, 'seagull': 110, 'turtle': 148, 'shark': 118}\n",
      "[0, 1] {'shark': 153, 'turtle': 156, 'dolphin': 164, 'seagull': 166}\n",
      "[0, 1] {'I': 127, 'B': 126, 'C': 115, 'D': 154, 'E': 118}\n",
      "[0, 1] {'shark': 153, 'turtle': 157, 'dolphin': 164, 'seagull': 166}\n",
      "[2, 3] {'G': 119, 'H': 127, 'A': 161, 'F': 124}\n",
      "[2, 3] {'dolphin': 155, 'seagull': 110, 'turtle': 148, 'shark': 118}\n",
      "[2, 3] {'G': 119, 'H': 127, 'A': 161, 'F': 125}\n",
      "[2, 3] {'dolphin': 156, 'seagull': 110, 'turtle': 148, 'shark': 118}\n",
      "[0, 1] {'I': 127, 'B': 126, 'C': 115, 'D': 154, 'E': 118}\n",
      "[0, 1] {'shark': 153, 'turtle': 157, 'dolphin': 164, 'seagull': 166}\n",
      "[0, 1] {'I': 127, 'B': 126, 'C': 116, 'D': 154, 'E': 118}\n",
      "[0, 1] {'shark': 154, 'turtle': 157, 'dolphin': 164, 'seagull': 166}\n",
      "[2, 3] {'dolphin': 156, 'seagull': 110, 'turtle': 148, 'shark': 118}\n",
      "[2, 3] {'G': 119, 'H': 127, 'A': 161, 'F': 125}\n",
      "[2, 3] {'G': 119, 'H': 127, 'A': 162, 'F': 125}\n",
      "[0, 1] {'I': 127, 'B': 126, 'C': 116, 'D': 154, 'E': 118}\n",
      "[2, 3] {'dolphin': 157, 'seagull': 110, 'turtle': 148, 'shark': 118}\n",
      "[0, 1] {'shark': 154, 'turtle': 157, 'dolphin': 164, 'seagull': 166}\n",
      "[2, 3] {'G': 119, 'H': 127, 'A': 163, 'F': 125}\n",
      "[0, 1] {'I': 127, 'B': 126, 'C': 116, 'D': 154, 'E': 118}\n",
      "[2, 3] {'dolphin': 158, 'seagull': 110, 'turtle': 148, 'shark': 118}\n",
      "[0, 1] {'shark': 154, 'turtle': 157, 'dolphin': 164, 'seagull': 166}\n",
      "[0, 1] {'I': 127, 'B': 126, 'C': 117, 'D': 154, 'E': 118}\n",
      "[0, 1] {'shark': 154, 'turtle': 158, 'dolphin': 164, 'seagull': 166}\n",
      "[2, 3] {'G': 119, 'H': 127, 'A': 163, 'F': 125}\n",
      "[2, 3] {'dolphin': 158, 'seagull': 110, 'turtle': 148, 'shark': 118}\n",
      "[0, 1] {'I': 127, 'B': 126, 'C': 117, 'D': 154, 'E': 119}\n",
      "[2, 3] {'G': 119, 'H': 127, 'A': 163, 'F': 125}\n",
      "[2, 3] {'dolphin': 158, 'seagull': 110, 'turtle': 148, 'shark': 118}\n"
     ]
    }
   ],
   "source": [
    "def animal_consumer(partitions=[]):\n",
    "    counts = {}   # key=animal, value=count\n",
    "    \n",
    "    consumer = KafkaConsumer(bootstrap_servers=[broker])\n",
    "    consumer.assign([TopicPartition(\"animals\", p) for p in partitions])\n",
    "    consumer.seek_to_beginning()\n",
    "    for i in range(10):      # TODO: loop forever\n",
    "        batch = consumer.poll(1000)\n",
    "        for tp, messages in batch.items():\n",
    "            for msg in messages:\n",
    "                s = Sighting.FromString(msg.value)\n",
    "\n",
    "                if not s.animal in counts:\n",
    "                    counts[s.animal] = 0\n",
    "                counts[s.animal] += 1\n",
    "        Print(partitions, counts)\n",
    "threading.Thread(target=animal_consumer, args=([0,1],)).start()\n",
    "threading.Thread(target=animal_consumer, args=([2,3],)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b0bcc-ab55-40a5-a186-4622ea6f49ff",
   "metadata": {},
   "source": [
    "# Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9c206aa-56e1-4aad-b631-85c9cea0672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, threading, json\n",
    "\n",
    "def animal_gen_json():\n",
    "    producer = KafkaProducer(bootstrap_servers=[broker])\n",
    "\n",
    "    while True:\n",
    "        beach = random.choice(list(\"ABCDEFGHI\"))\n",
    "        animal = random.choice([\"shark\", \"dolphin\", \"turtle\", \"seagull\"])\n",
    "\n",
    "        value = bytes(json.dumps({\"beach\": beach, \"animal\": animal}), \"utf-8\")\n",
    "        producer.send(\"animals-json\", value=value, key=bytes(beach, \"utf-8\"))\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "threading.Thread(target=animal_gen_json).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8339f693-ab99-4e4e-8040-63033079e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session (with Kafka jar)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"demo\")\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0')\n",
    "         .config(\"spark.sql.shuffle.partitions\", 10) # important\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70f52414-df72-48cb-b851-254a12739ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", broker)\n",
    "    .option(\"subscribe\", \"animals-json\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "492646ac-c942-45fb-8793-df6c6e5742ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20e78ed8-3587-4789-9e51-9914158513c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 'binary'),\n",
       " ('value', 'binary'),\n",
       " ('topic', 'string'),\n",
       " ('partition', 'int'),\n",
       " ('offset', 'bigint'),\n",
       " ('timestamp', 'timestamp'),\n",
       " ('timestampType', 'int')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3a1301c-75d8-4136-b9be-8b13afca3726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/14 05:42:37 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e62cfc62-9faa-44ae-ad34-2a201a331c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/14 05:42:37 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1016"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb6f3dd4-3bdc-4f2a-b89d-93e673cf7f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/14 05:42:38 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "      <th>topic</th>\n",
       "      <th>partition</th>\n",
       "      <th>offset</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timestampType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[73]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-14 05:13:50.472</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[67]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-14 05:13:55.479</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[73]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-14 05:13:56.480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[67]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-01-14 05:13:59.483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[73]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2024-01-14 05:14:08.494</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    key                                              value         topic  \\\n",
       "0  [73]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "1  [67]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "2  [73]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "3  [67]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "4  [73]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "\n",
       "   partition  offset               timestamp  timestampType  \n",
       "0          0       0 2024-01-14 05:13:50.472              0  \n",
       "1          0       1 2024-01-14 05:13:55.479              0  \n",
       "2          0       2 2024-01-14 05:13:56.480              0  \n",
       "3          0       3 2024-01-14 05:13:59.483              0  \n",
       "4          0       4 2024-01-14 05:14:08.494              0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73b59cc6-c4c9-4acd-ad85-56ad3809763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72e05d50-f841-45c7-a0ca-1aeefb6c2b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/14 05:42:38 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>beach</th>\n",
       "      <th>animal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>dolphin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>turtle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>turtle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>seagull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>seagull</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  key beach   animal\n",
       "0   I     I  dolphin\n",
       "1   C     C   turtle\n",
       "2   I     I   turtle\n",
       "3   C     C  seagull\n",
       "4   I     I  seagull"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = \"beach string, animal string\"\n",
    "\n",
    "animals = (\n",
    "    df\n",
    "    .select(col(\"key\").cast(\"string\"), col(\"value\").cast(\"string\"))\n",
    "    .select(\"key\", from_json(\"value\", schema).alias(\"value\"))\n",
    "    .select(\"key\", \"value.*\")\n",
    ")\n",
    "animals.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa197550-ab9a-439d-a71f-62bab4524c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, beach: string, animal: string]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = \"beach string, animal string\"\n",
    "\n",
    "animals = (\n",
    "    df\n",
    "    .select(col(\"key\").cast(\"string\"), col(\"value\").cast(\"string\"))\n",
    "    .select(\"key\", from_json(\"value\", schema).alias(\"value\"))\n",
    "    .select(\"key\", \"value.*\")\n",
    ")\n",
    "animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fdce1d54-16e0-459b-aaf5-3ad2bd04ac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/14 05:42:39 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>beach</th>\n",
       "      <th>animal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>dolphin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>turtle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>turtle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>seagull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>seagull</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  key beach   animal\n",
       "0   I     I  dolphin\n",
       "1   C     C   turtle\n",
       "2   I     I   turtle\n",
       "3   C     C  seagull\n",
       "4   I     I  seagull"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5c84669-d54e-479b-9ffa-ab4aef9c41d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/14 05:42:39 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1019"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e1e80dc-509f-4a6a-8c08-ca8b3d0d6508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c43444-81f8-472f-b81c-1040de0dd58b",
   "metadata": {},
   "source": [
    "# Streaming DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d7ba406-af30-44b2-b0b4-a53f0a95be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source => transformations => sink\n",
    "# streaming_query = spark.readStream(????).????.writeStream(????)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed6e1956-a3b3-43ee-bc55-b13ac280e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.readStream.format(\"kafka\") # instead of spark.read.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", broker)\n",
    "    .option(\"subscribe\", \"animals-json\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79e5ed09-6c6f-4b89-b3c9-f046f4de2456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2be8c8a1-99f0-4e06-bfc5-827578a71b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87198d60-46b7-4a41-b038-5b5df8015219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, beach: string, animal: string]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = \"beach string, animal string\"\n",
    "\n",
    "animals = (\n",
    "    df\n",
    "    .select(col(\"key\").cast(\"string\"), col(\"value\").cast(\"string\"))\n",
    "    .select(\"key\", from_json(\"value\", schema).alias(\"value\"))\n",
    "    .select(\"key\", \"value.*\")\n",
    ")\n",
    "animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b4d10a2f-71b9-45d3-ab83-6de59c1552c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not supported for streaming\n",
    "# animals.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c999e5e-e406-40bc-904c-e9b5c07865fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not supported for streaming\n",
    "# animals.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3823de2a-39ac-4070-a021-27aaea5af87b",
   "metadata": {},
   "source": [
    "# Shark Alert App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc05ed91-8f39-473b-861f-907d26ac5d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/14 05:42:40 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4626da84-afb9-4200-bcc3-3e9a7e099421. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/01/14 05:42:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.streaming.query.StreamingQuery"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_query = (\n",
    "    animals\n",
    "    .filter(\"animal='shark'\")\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .outputMode(\"append\")\n",
    ").start()\n",
    "type(streaming_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe6d8e06-a2d8-4a50-9ee3-fbb6b8dd573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.active[0].stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "660e5254-037c-4c72-970f-30dff81e54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query.stop()\n",
    "# spark.streams.active[0].stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f3750-8f0d-4592-a1f2-128ae6900141",
   "metadata": {},
   "source": [
    "# Animal Counter App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a21fbea-6184-4e42-9b13-b6f14aaa46e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/14 05:42:40 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ad866f28-68b3-4450-93e7-a99688ad0f22. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/01/14 05:42:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "q = (\n",
    "    animals.groupby(\"animal\").count()\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .outputMode(\"complete\")\n",
    ").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee0e0d51-782b-43c1-90cb-74329a02671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
